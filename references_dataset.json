[{"paper": "ksk.pdf", "references": "REFERENCES \n[1] \nT. Ohno, N. Mukawa and A. Yoshoikawa. FREEGAZE: a gaze tracking \nsystem for everyday dazes interaction, Proceedings of the Symposium on \nETRA 2002: Eye Tracking Research and Applications Symposium, pp. \n125-132, 2002. \n[2] \nRhys Newman and Alexander Zelinsky. Error analysis of head poses and \ngaze direction from stereo vision, Proceedings of the Australian Conf. on \nRobotics and Automation, pp. 114-118, Mar. 1999.  \n[3] \nhttp://www.seeing machines.com/facelab.htm \n[4] \nS.W. Shih et al. A novel Approach to 3-D Gaze Tracking sing Stereo \nCameras, IEEE Trans. on Systems, Man, and Cybernetics part B: \nCybernetics, vol. 34, no. 1, Feb, 2004. \n[5] \nShelly L. Ames and Neville A. McBrien. Development of a miniaturized \nsystem for monitoring vergence during viewing of stereoscopic imagery \nusing a head mounted display, Proceedings Of SPIE IS&T Electronics \nImaging, SPIE vol. 5291, pp. 25-35, 2004. \n[6] \nKai Essig, Marc Pomplun and Helge Ritter. Application of a novel neural \napproach \nto \n3D \ngaze \ntracking: \nvergence \neye-movements \nin \nautostereograms, Proceedings of the 26th Annual Meeting of the \nCognitive Science Society, Chicago, USA, Aug. 2004. \n[7] \nKai Essig, Marc Pomplun and Helge Ritter. A neural network for 3D gaze \nrecording with binocular eye trackers, Int'l Journal of Parallel, Emergent \nand Distributed Systems, vol. 21, no. 2, pp. 79-95, Apr. 2006."}, {"paper": "Segment-Anything.pdf", "references": "References\n[1] Edward H Adelson. On seeing stuff: the perception of materials by\nhumans and machines. Human vision and electronic imaging VI,\n2001. 5\n[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an\nobject? CVPR, 2010. 4, 10\n[3] Pablo Arbel\u00b4aez, Michael Maire, Charless Fowlkes, and Jitendra\nMalik.\nContour detection and hierarchical image segmentation.\nTPAMI, 2010. 4, 10, 21, 28\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer\nnormalization. arXiv:1607.06450, 2016. 16\n[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of\nimage transformers. arXiv:2106.08254, 2021. 17\n[6] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl,\nFadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel\nBargal, and Kate Saenko. ZeroWaste dataset: Towards deformable\nobject segmentation in cluttered scenes. CVPR, 2022. 9, 20\n[7] Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N.\nStraehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,\nJanez Ales, Thorsten Beier, Markus Rudy, Kemal Eren, Jaime I.\nCervantes, Buote Xu, Fynn Beuttenmueller, Adrian Wolny, Chong\nZhang, Ullrich Koethe, Fred A. Hamprecht, and Anna Kreshuk.\nilastik: interactive machine learning for (bio)image analysis. Na-\nture Methods, 2019. 12\n[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,\nSimran Arora, Sydney von Arx, Michael S Bernstein, Jeannette\nBohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv:2108.07258, 2021. 1,\n12\n[9] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative\ninteraction training for segmentation editing networks. MICCAI,\n2018. 17\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. NeurIPS, 2020. 1, 4\n[11] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into\nhigh quality object detection. CVPR, 2018. 10\n[12] Juan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Ci-\nmini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim\nBecker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-\ntanu Singh, and Anne E. Carpenter. Nucleus segmentation across\nimaging experiments: the 2018 data science bowl. Nature Methods,\n2019. 9, 19, 20\n[13] John Canny. A computational approach to edge detection. TPAMI,\n1986. 10, 21\n[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end\nobject detection with Transformers. ECCV, 2020. 5, 16, 17\n[15] Guillaume Charpiat, Matthias Hofmann, and Bernhard Sch\u00a8olkopf.\nAutomatic image"}, {"paper": "LSTM.pdf", "references": "References\nAlmeida,\nL.\nB.\n(\u0001\t\b\u0007).\nA\nlearning\nrule\nfor\nasync\nhronous\np\nerceptrons\nwith\nfeedbac\nk\nin\na\ncom\nbina-\ntorial\nen\nvironmen\nt.\nIn\nIEEE\n\u0001st\nInternational\nConfer\nenc\ne\non\nNeur\nal\nNetworks,\nSan\nDie\ngo,\nv\nolume\n\u0002,\npages\n\u00060\t{\u0006\u0001\b.\nBaldi,\nP\n.\nand\nPineda,\nF.\n(\u0001\t\t\u0001).\nCon\ntrastiv\ne\nlearning\nand\nneural\noscillator.\nNeur\nal\nComputation,\n\u0003:\u0005\u0002\u0006{\u0005\u0004\u0005.\nBengio,\nY.\nand\nF\nrasconi,\nP\n.\n(\u0001\t\t\u0004).\nCredit\nassignmen\nt\nthrough\ntime:\nAlternativ\nes\nto\nbac\nkpropaga-\ntion.\nIn\nCo\nw\nan,\nJ.\nD.,\nT\nesauro,\nG.,\nand\nAlsp\nector,\nJ.,\neditors,\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0006,\npages\n\u0007\u0005{\b\u0002.\nSan\nMateo,\nCA:\nMorgan\nKaufmann.\nBengio,\nY.,\nSimard,\nP\n.,\nand\nF\nrasconi,\nP\n.\n(\u0001\t\t\u0004).\nLearning\nlong-term\ndep\nendencies\nwith\ngradien\nt\ndescen\nt\nis\ndi\ufffdcult.\nIEEE\nT\nr\nansactions\non\nNeur\nal\nNetworks,\n\u0005(\u0002):\u0001\u0005\u0007{\u0001\u0006\u0006.\nCleeremans,\nA.,\nServ\nan-Sc\nhreib\ner,\nD.,\nand\nMcClelland,\nJ.\nL.\n(\u0001\t\b\t).\nFinite-state\nautomata\nand\nsimple\nrecurren\nt\nnet\nw\norks.\nNeur\nal\nComputation,\n\u0001:\u0003\u0007\u0002{\u0003\b\u0001.\nde\nV\nries,\nB.\nand\nPrincip\ne,\nJ.\nC.\n(\u0001\t\t\u0001).\nA\ntheory\nfor\nneural\nnet\nw\norks\nwith\ntime\ndela\nys.\nIn\nLippmann,\nR.\nP\n.,\nMo\no\ndy\n,\nJ.\nE.,\nand\nT\nouretzky\n,\nD.\nS.,\neditors,\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0003,\npages\n\u0001\u0006\u0002{\u0001\u0006\b.\nSan\nMateo,\nCA:\nMorgan\nKaufmann.\nDo\ny\na,\nK.\n(\u0001\t\t\u0002).\nBifurcations\nin\nthe\nlearning\nof\nrecurren\nt\nneural\nnet\nw\norks.\nIn\nPr\no\nc\ne\ne\ndings\nof\n\u0001\t\t\u0002\nIEEE\nInternational\nSymp\nosium\non\nCir\ncuits\nand\nSystems,\npages\n\u0002\u0007\u0007\u0007{\u0002\u0007\b0.\nDo\ny\na,\nK.\nand\nY\noshiza\nw\na,\nS.\n(\u0001\t\b\t).\nAdaptiv\ne\nneural\noscillator\nusing\ncon\ntin\nuous-time\nbac\nk-\npropagation\nlearning.\nNeur\nal\nNetworks,\n\u0002:\u0003\u0007\u0005{\u0003\b\u0005.\nElman,\nJ.\nL.\n(\u0001\t\b\b).\nFinding\nstructure\nin\ntime.\nT\nec\nhnical\nRep\nort\nCRL\nT\nec\nhnical\nRep\nort\n\b\b0\u0001,\nCen\nter\nfor\nResearc\nh\nin\nLanguage,\nUniv\nersit\ny\nof\nCalifornia,\nSan\nDiego.\nF\nahlman,\nS.\nE.\n(\u0001\t\t\u0001).\nThe\nrecurren\nt\ncascade-correlation\nlearning\nalgorithm.\nIn\nLippmann,\nR.\nP\n.,\nMo\no\ndy\n,\nJ.\nE.,\nand\nT\nouretzky\n,\nD.\nS.,\neditors,\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0003,\npages\n\u0001\t0{\u0001\t\u0006.\nSan\nMateo,\nCA:\nMorgan\nKaufmann.\nHo\nc\nhreiter,\nJ.\n(\u0001\t\t\u0001).\nUn\ntersuc\nh\nungen\nzu\ndynamisc\nhen\nneuronalen\nNetzen.\nDiploma\nthe-\nsis,\nInstitut\nf\n\ufffd\nur\nInformatik,\nLehrstuhl\nProf.\nBrauer,\nT\nec\nhnisc\nhe\nUniv\nersit\ufffd\nat\nM\n\ufffd\nunc\nhen.\nSee\nwww\u0007.informatik.tu-m\nuenc\nhen.de/~ho\nc\nhreit.\n\u00030\nHo\nc\nhreiter,\nS.\nand\nSc\nhmidh\nub\ner,\nJ.\n(\u0001\t\t\u0005).\nLong\nshort-term\nmemory\n.\nT\nec\nhnical\nRep\nort\nFKI-\u00020\u0007-\n\t\u0005,\nF\nakult\ufffd\nat\nf\n\ufffd\nur\nInformatik,\nT\nec\nhnisc\nhe\nUniv\nersit\ufffd\nat\nM\n\ufffd\nunc\nhen.\nHo\nc\nhreiter,\nS.\nand\nSc\nhmidh\nub\ner,\nJ.\n(\u0001\t\t\u0006).\nBridging\nlong\ntime\nlags\nb\ny\nw\neigh\nt\nguessing\nand\n\\Long\nShort-T\nerm\nMemory\".\nIn\nSilv\na,\nF.\nL.,\nPrincip\ne,\nJ.\nC.,\nand\nAlmeida,\nL.\nB.,\neditors,\nSp\na-\ntiotemp\nor\nal\nmo\ndels\nin\nbiolo\ngic\nal\nand\narti\ufffdcial\nsystems,\npages\n\u0006\u0005{\u0007\u0002.\nIOS\nPress,\nAmsterdam,\nNetherlands.\nSerie:\nF\nron\ntiers\nin\nArti\ufffdcial\nIn\ntelligence\nand\nApplications,\nV\nolume\n\u0003\u0007.\nHo\nc\nhreiter,\nS.\nand\nSc\nhmidh\nub\ner,\nJ.\n(\u0001\t\t\u0007).\nLSTM\ncan\nsolv\ne\nhard\nlong\ntime\nlag\nproblems.\nIn\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\t.\nMIT\nPress,\nCam\nbridge\nMA.\nPresen\nted\nat\nNIPS\n\t\u0006.\nLang,\nK.,\nW"}, {"paper": "ResNet.pdf", "references": "References\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks, 5(2):157\u2013166, 1994.\n[2] C. M. Bishop.\nNeural networks for pattern recognition.\nOxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\n2000.\n[4] K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nIn BMVC, 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303\u2013338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.\n[9] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training\ndeep feedforward neural networks. In AISTATS, 2010.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR, 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV, 2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. In\nICCV, 2015.\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580, 2012.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735\u20131780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML, 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI, 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI, 2012.\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093, 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report, 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS, 2012.\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation, 1989.\n[23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. Ef\ufb01cient backprop.\nIn Neural Networks: Tricks of the Trade, page"}, {"paper": "transformer.pdf", "references": "References\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn Internat"}]